## ——————2018.4.5——————

该repo是我在学习CS231n过程中，对所有notes和homework中包含的代码的编写，并加入了详细的comments。  
README中是我的一些笔记和感想



## ——————2018.4.8—————— 
完成Note1_NN，所有部分都做了详细标注，学习了怎么导入数据，发现python挺慢的

完成assignment1中的部分KNN

其中计算distance部分，完全不用loop没有想出来，借鉴一下网上的答案，本质上用了一个数学上平方展开加上矩阵广播，比较巧妙。

学习numpy库中的：  
np.argsort（将矩阵中的元素按行或列从小到大排列，输出其对应的标号值）  
np.bincount（输入一维矩阵，输出矩阵各元素出现的次数）  
np.argmax（返回对应维度的最大索引）  
np.linalg.normal（求输入矩阵的二范数，默认）  


## ——————2018.4.10——————
完成knn.ipynb

学习numpy库中的：  
np.split(x,n), (x, [])（将行矩阵平均分成n个，或者按照list分成几份）  
np.hstack()（水平的按列对数组或者矩阵进行堆叠）  
np.vstack()（竖直的按行对数组或者矩阵进行堆叠）

制作交叉验证集时注意使用list嵌套list

时刻注意函数的输入 是什么，矩阵的形状


## ——————2018.4.12—————— 
SVM分类器的损失函数鼓励正确的分类的分值比其他分类的分值高出至少一个边界值。  
而softmax是计算所有类别的可能性，同时softmax分类器对于分数是永远不会满意的：正确分类总能得到更高的可能性，错误分类总能得到更低的可能性  
个人觉得SVM方法在实现的过程中还是有些绕的，特别是理论求导修正W的部分，容易弄错

寻找矩阵中的元素可采用 a[list1, list2]　的方法


linear_svm时，发现对其中dW的计算理解的还不是很到位，注意每一个样本进去，所有的W都将更新，而不仅仅更新W[:, y[i]] 

(a>0).astype(int) / (float), 让矩阵中a>0的设为1，小于0的设为0

## ——————2018.4.13——————

在写的过程中用矩阵实现计算梯度始终不是很会，借鉴了别人的写法，但还是很难想

``` python
  counts = (margin > 0).astype(int)
  counts[range(N), y] = - np.sum(counts, axis = 1)
  dW += np.dot(X.T, counts) / N + reg * W
```

np.random.choice在使用时先求随机索引会方便很多  
np.argmax() 寻找矩阵中的最大值索引

svm.ipynb中源代码好像不太对，for x in results()改成for x in results.values()，访问存放的具体值


很明显能感觉到softmax的编程实现比SVM清晰很多，不需要一个一个对比

发现网上别人开源的作业源码的一处错误，很奇怪他是怎么编译成功的
 
```
for j in range(num_class):
	if j == y[i]:
        	continue
        dW[:, j] += X[i].T * np.exp(score[j]) / np.sum(np.exp(score))
```

原作者没有加转置符号

## ——————2018.4.14——————

注意理解反向传播中的局部过程：  
每个门单元都会得到一些输入并立即计算两个东西：1. 这个门的输出值和2.其输出值关于输入值的局部梯度。门单元完成这两件事是完全独立的，它不需要知道计算线路中的其他细节。而之后反向传播时门先得到最终输出关于该门输出的梯度，再用链式法则求关于上一级输出的梯度

sigmoid函数求导为 (1-sigmoid)*sigmoid

numpy.random.rand()：根据给定维度生成[0,1)之间的数，且正太分布  
numpy.random.random(())：根据给定维度生成[0,1)之间的随机浮点  
np.random.normal（均值，方差，大小）：和上面一个相比多了几个参数

sigmoid函数使用缺点：  
激活值在接近0,1时，梯度几乎为0，即局部梯度几乎为0，导致反向传播时使总的梯度接近0  
这样初始权重的时候也要特别注意，如果初始的过大，就有很多的神经元饱和，不再学习  
sigmoid函数输出不是零中心的，这样接下来的输入也不是零中心的

relu变得流行：f(x) = max(0, x)  
对随机梯度下降有巨大加速作用，但缺点是比较脆弱，注意设置好学习率

单层神经网络可以近似任何连续函数，但深层网络明显更好，同时正则化可以很好的防止过拟合

进行预处理很重要的一点是：任何预处理策略（比如数据均值）都只能在训练集数据上进行计算，算法训练完毕后再应用到验证集或者测试集上。

注意权重不能进行全零初始化，而是小随机数初始化，W = 0.01 * np.random.randn(D,H)

两层的神经网络完成过程中，弄错了W的大小，预先看好维度很重要！！！

按照作业的要求尝试调了一下参，感觉还是比较没有头绪的，只能根据前一次的结果缩小范围，但这对复杂庞大的数据集来说肯定不是好办法。

## ——————2018.4.17——————

开始学习assignmen2

学习np库  
np.prod((数字))表示连乘  
np.linspace(a, b, c)从a到b均匀返回c个数字  

python中的匿名函数

体会到神经网络中的模块化编写带来的便利！但要注意反向传播时dw要加上 reg×w  







